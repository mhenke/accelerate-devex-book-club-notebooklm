# Meeting 1 Discussion Questions Resource Bank

**Laying the Foundation – The Myth and the Measurement**

## Purpose of This Document

This is a **question bank** containing 40+ discussion questions for Meeting 1. **You will NOT use all of these questions.** Facilitators should select 6-8 questions that best fit their team's context for the **25-minute core discussion period** (the full 60-minute meeting includes other activities per the agenda).

See [Quick Start Guide](#quick-start-recommended-meeting-flow) below for a default meeting flow, or browse the [Question Bank](#question-bank-all-options) to customize your discussion.

---

## Core Framing Question

> **See [Key Questions - Meeting 1](../../../key-questions.md#meeting-1)**

**"How do we currently _perceive_ and _measure_ performance, and what would it take to shift our focus from local outputs (like utilization or story points) to the global outcomes of speed and stability that the research proves matter?"**

This question challenges common metrics (lines of code, velocity) and prompts evaluation of current practices against evidence-based models. It connects measurement (Ch. 2) to the cultural environment needed for learning instead of blame (Ch. 3).

---

## Quick Start: Recommended Meeting Flow

**Full Meeting: 60 minutes (25 min core discussion + 35 min other activities) | Questions Used: 6-8**

*Note: The agenda allocates 25 minutes for core discussion. Opening/wrap-up happen in other agenda sections (welcome, action items).*

### Part 1: Understanding DORA Metrics (10 min)
- **Question 1:** What metrics does your organization currently use to assess software delivery performance?
- **Question 3:** If you could only track one metric starting tomorrow, which DORA metric would you choose and why?
- **Question 17:** Which DORA metric would be easiest to improve? Which would be hardest?

### Part 2: Speed vs. Stability Myth (10 min)
- **Question 13:** Before reading the book, did you believe there was a trade-off between speed and stability?
- **Question 14:** How does the research finding (46x, 440x, 170x) challenge assumptions in your organization?
- **Question 16:** Can you think of a time when slowing down deployment actually made things worse?

### Part 3: Culture & Change (5 min)
- **Question 9:** Using Westrum's model, what culture type best describes your organization?
- **Question 21:** Do you believe culture can be changed through implementing technical practices?

---

## Question Bank: All Options

**Instructions:** Browse questions below and select 6-8 that fit your team's context for the **25-minute core discussion**. The [Quick Start Guide](#quick-start-recommended-meeting-flow) above provides a recommended selection for a typical meeting.

### Section A: Understanding Current State

**On Performance Measurement:**
1. What metrics does your organization currently use to assess software delivery performance?
2. How do these metrics influence team behavior? Are the influences positive or negative?
3. If you could only track one metric starting tomorrow, which DORA metric would you choose and why?
4. What barriers prevent you from measuring the four DORA metrics today?

**On Deployment Practices:**
5. Describe your team's typical deployment process. How long does it take from "code complete" to "in production"?
6. On a scale of 1-10, how anxious are you before a production deployment? What specific factors contribute to that anxiety?
7. What deployment pain points cause the most stress for your team?
8. When was the last time a deployment went smoothly? What made it different?

**On Organizational Culture:**
9. Using Westrum's model, what culture type best describes your organization: pathological, bureaucratic, or generative?
10. Give a concrete example of how information flows (or doesn't flow) in your organization.
11. What happens when someone brings bad news to leadership or management?
12. Can you identify specific behaviors that indicate your organization's culture type?

### Section B: Exploring Concepts

**On the Speed vs. Stability Myth:**
13. Before reading the book, did you believe there was a trade-off between speed and stability? What examples influenced that belief?
14. How does the research finding (46x, 440x, 170x) challenge assumptions in your organization?
15. What quality practices does your team already have that support both speed and stability?
16. Can you think of a time when slowing down deployment actually made things worse, not better?

**Follow-up prompts:**
- "What would it take to prove this in your organization?"
- "Who needs to see this evidence?"
- "What practices create both speed and stability?"

**On DORA Metrics:**
17. Which DORA metric would be easiest to improve in your organization? Which would be hardest?
18. How might focusing on these metrics change team priorities or behaviors?
19. What concerns do you have about measuring these metrics?
20. How could you use these metrics for learning vs. performance evaluation?

**Follow-up prompts:**
- "What would good performance look like?"
- "What's blocking improvement?"
- "How could we measure this without expensive tools?"

**On Culture Change:**
21. Do you believe culture can be changed through implementing technical practices? Why or why not?
22. What's one technical practice that might positively shift your team's culture toward more generative?
23. What obstacles would you face trying to implement that practice?
24. How does your current culture support or hinder software delivery performance?

**Follow-up prompts:**
- "What would need to change first?"
- "Who has influence over this?"
- "What small experiment could we try?"

### Section C: Real-World Scenarios

**Scenario 1: The Metrics Debate**
Your manager wants to start tracking individual developer productivity using lines of code and story points completed. How would you use concepts from chapters 1-3 to propose an alternative?

**Discussion points:**
- Why are individual metrics problematic?
- What system-level metrics better predict success?
- How to frame conversation with manager?

**Scenario 2: The Deployment Incident**
Last Friday's deployment caused a production outage. Leadership is proposing more approval gates and longer testing cycles to prevent future incidents. How would the book's research inform your response?

**Discussion points:**
- Does slowing down improve stability?
- What actually prevents deployment failures?
- How to propose evidence-based alternative?

**Scenario 3: The Culture Clash**
Your team wants to adopt trunk-based development, but another team insists on long-lived feature branches and formal merge approvals. Both teams must share the same codebase. How do you navigate this?

**Discussion points:**
- What culture types might each team represent?
- How does culture impact technical choices?
- What conversation might bridge the gap?

### Section D: Team Self-Assessment

**Deployment Frequency:**
25. How often does your team deploy to production?
   - Multiple times per day
   - Daily to weekly
   - Weekly to monthly
   - Monthly to less frequently

26. What prevents more frequent deployments?

**Lead Time:**
27. How long from commit to production deployment?
   - Less than 1 hour
   - 1 day to 1 week
   - 1 week to 1 month
   - More than 1 month

28. Where is time lost in that process?

**Time to Restore:**
29. When production breaks, how quickly can you restore service?
   - Less than 1 hour
   - 1 hour to 1 day
   - 1 day to 1 week
   - More than 1 week

30. What slows down recovery?

**Change Fail Rate:**
31. What percentage of deployments cause production issues?
   - 0-15%
   - 15-30%
   - 30-45%
   - More than 45%

32. What typically causes deployment failures?

### Section E: Reflection and Commitment

**Personal Reflection:**
33. What surprised you most in chapters 1-3?
34. What concept challenged your existing beliefs?
35. What evidence do you need to see to believe this applies to your context?
36. What's your biggest takeaway from Meeting 1?

**Team Commitment:**
37. Which DORA metric will you start tracking before Meeting 2?
38. How will you track it? (tool, spreadsheet, manual observation)
39. What deployment pain point will you document or address?
40. What culture behavior will you observe over the next two weeks?

---

## How to Customize Your Discussion

### For Facilitators

**Step 1: Choose Your Approach**
- **Option A:** Use the [Quick Start Guide](#quick-start-recommended-meeting-flow) above (8 pre-selected questions for 25 min)
- **Option B:** Build custom discussion by selecting 6-8 questions from the [Question Bank](#question-bank-all-options)

**Step 2: Selection Guidelines (for 25-minute core discussion)**
- **Start with current state:** Select 2-3 questions from Section A to ground discussion in team's experience (10 min)
- **Focus on 1 key concept:** Choose either speed/stability myth OR culture change for depth (10 min)
- **Close with reflection:** 1-2 questions from Section E if time permits (5 min)
- **Skip scenarios:** Scenarios add 10-15 minutes and won't fit in 25-minute discussion slot

**Step 3: Facilitation Tips**
- **Use follow-up prompts** provided in each section to deepen superficial answers
- **Reference book statistics** (46x, 440x, 170x) when challenging assumptions
- **Capture commitments** in writing - these become Meeting 2 accountability check-ins
- **If running short on time:** Skip scenarios, keep reflection questions

### For Participants

- **Come prepared** with specific examples from your work (deployment stories, culture observations)
- **Be honest** about current state - psychological safety is essential for real discussion
- **Challenge assumptions** constructively - this is a learning space
- **Connect to evidence** from chapters 1-3 when making points
- **Commit to action** - you'll be asked which metric you'll track before Meeting 2

### Meeting Time Allocation (60 minutes)

**Full Meeting Structure per Agenda:**
- Welcome & Icebreaker: 5 min (from agenda)
- Key Concepts Review: 10 min (from agenda)
- **Core Discussion: 25 min** (this is where you use discussion questions)
- AI Tool Spotlight: 5 min (from agenda)
- Action Items & Commitments: 10 min (from agenda)
- Wrap-Up & Preview: 5 min (from agenda)

**Core Discussion Time (25 minutes) - Using Quick Start Guide:**
- Part 1 (DORA Metrics): 10 min (3 questions)
- Part 2 (Speed vs Stability): 10 min (3 questions)
- Part 3 (Culture): 5 min (2 questions)

**Custom Discussion Flow (25 minutes):**
- Current State: 10 min (2-3 questions from Section A)
- Core Concepts: 10 min (2-3 questions from Section B)
- Culture OR Scenarios: 5 min (1-2 questions from Section C or D)

---

## Related Assessments

Use these tools during or after discussion:

- **[DORA Metrics Template](../../../assessments/dora-metrics-template.md)** - Begin baseline tracking
- **[Culture Assessment Worksheet](../../../assessments/culture-assessment-worksheet.md)** - Identify current culture type
- **[24 Capabilities Assessment](../../../assessments/24-capabilities-assessment.md)** - Complete organizational baseline

---

## Next Meeting Prep

**[Meeting 2: Technical Excellence & Continuous Delivery](../../meeting-2/)**

Preview question to consider:
> "Which of our current architectural or security practices create the most friction and dependencies for our teams?"

---

[← Meeting 1 Resources](../README.md) | [Key Questions](../../../key-questions.md) →
